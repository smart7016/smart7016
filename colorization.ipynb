{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colorization.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7Y6KFeRA0mOV","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","from torch import nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.datasets import LSUN\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jd5ilqnn1ayb","colab_type":"code","outputId":"870dc789-fa0a-4b03-cb38-731d9330db5a","executionInfo":{"status":"ok","timestamp":1565496035616,"user_tz":-540,"elapsed":4862,"user":{"displayName":"seunghyung lee","photoUrl":"","userId":"13129293040216281247"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["!pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"daK6JcyV1ek1","colab_type":"code","outputId":"c1f23183-26a8-4ff1-b63f-5e5a2995a10d","executionInfo":{"status":"ok","timestamp":1565612752850,"user_tz":-540,"elapsed":29103,"user":{"displayName":"seunghyung lee","photoUrl":"","userId":"13129293040216281247"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tD0B4_JF2dC0","colab_type":"code","colab":{}},"source":["from torch import nn\n","class VAE(nn.Module):\n","  \n","  #define layers\n","  def __init__(self):\n","    super(VAE, self).__init__()\n","    self.hidden_size = 256\n","\n","    #Encoder layers\n","    self.enc_conv1 = nn.Conv2d(3, 128, 5, stride=2, padding=2) # rgb\n","    self.enc_bn1 = nn.BatchNorm2d(128)\n","    self.enc_conv2 = nn.Conv2d(128, 256, 5, stride=2, padding=2)\n","    self.enc_bn2 = nn.BatchNorm2d(256)\n","    self.enc_conv3 = nn.Conv2d(256, 512, 5, stride=2, padding=2)\n","    self.enc_bn3 = nn.BatchNorm2d(512)\n","    self.enc_conv4 = nn.Conv2d(512, 1024, 3, stride=2, padding=1)\n","    self.enc_bn4 = nn.BatchNorm2d(1024)\n","    self.enc_fc1 = nn.Linear(4*4*1024, self.hidden_size*2)\n","    self.enc_dropout1 = nn.Dropout(p=.7)\n","\n","    #Cond encoder layers\n","    self.cond_enc_conv1 = nn.Conv2d(1, 128, 5, stride=2, padding=2)\n","    self.cond_enc_bn1 = nn.BatchNorm2d(128)\n","    self.cond_enc_conv2 = nn.Conv2d(128, 256, 5, stride=2, padding=2)\n","    self.cond_enc_bn2 = nn.BatchNorm2d(256)\n","    self.cond_enc_conv3 = nn.Conv2d(256, 512, 5, stride=2, padding=2)\n","    self.cond_enc_bn3 = nn.BatchNorm2d(512)\n","    self.cond_enc_conv4 = nn.Conv2d(512, 1024, 3, stride=2, padding=1)\n","    self.cond_enc_bn4 = nn.BatchNorm2d(1024)\n","\n","    #Decoder layers\n","    self.dec_upsamp1 = nn.Upsample(scale_factor=4, mode='bilinear')\n","    self.dec_conv1 = nn.Conv2d(1024+self.hidden_size, 512, 3, stride=1, padding=1)\n","    self.dec_bn1 = nn.BatchNorm2d(512)\n","    self.dec_upsamp2 = nn.Upsample(scale_factor=2, mode='bilinear')\n","    self.dec_conv2 = nn.Conv2d(512*2, 256, 5, stride=1, padding=2)\n","    self.dec_bn2 = nn.BatchNorm2d(256)\n","    self.dec_upsamp3 = nn.Upsample(scale_factor=2, mode='bilinear')\n","    self.dec_conv3 = nn.Conv2d(256*2, 128, 5, stride=1, padding=2)\n","    self.dec_bn3 = nn.BatchNorm2d(128)\n","    self.dec_upsamp4 = nn.Upsample(scale_factor=2, mode='bilinear')\n","    self.dec_conv4 = nn.Conv2d(128*2, 64, 5, stride=1, padding=2)\n","    self.dec_bn4 = nn.BatchNorm2d(64)\n","    self.dec_upsamp5 = nn.Upsample(scale_factor=2, mode='bilinear')\n","    self.dec_conv5 = nn.Conv2d(64, 3, 5, stride=1, padding=2) #rgb\n","\n","  def encoder(self, x):\n","    x = F.relu(self.enc_conv1(x))\n","    x = self.enc_bn1(x)\n","    x = F.relu(self.enc_conv2(x))\n","    x = self.enc_bn2(x)\n","    x = F.relu(self.enc_conv3(x))\n","    x = self.enc_bn3(x)\n","    x = F.relu(self.enc_conv4(x))\n","    x = self.enc_bn4(x)\n","    x = x.view(-1, 4*4*1024)\n","    \n","    x = self.enc_fc1(x)\n","    mu = x[..., :self.hidden_size]\n","    logvar = x[..., self.hidden_size:]\n","    return mu, logvar\n","\n","  def cond_encoder(self, x):\n","    x = F.relu(self.cond_enc_conv1(x))\n","    sc_feat32 = self.cond_enc_bn1(x)\n","    x = F.relu(self.cond_enc_conv2(sc_feat32))\n","    sc_feat16 = self.cond_enc_bn2(x)\n","    x = F.relu(self.cond_enc_conv3(sc_feat16))\n","    sc_feat8 = self.cond_enc_bn3(x)\n","    x = F.relu(self.cond_enc_conv4(sc_feat8))\n","    sc_feat4 = self.cond_enc_bn4(x)\n","    return sc_feat32, sc_feat16, sc_feat8, sc_feat4\n","\n","  def decoder(self, z, sc_feat32, sc_feat16, sc_feat8, sc_feat4):\n","    x = z.view(-1, self.hidden_size, 1, 1)\n","    x = self.dec_upsamp1(x)\n","    x = torch.cat([x, sc_feat4], 1)\n","    x = F.relu(self.dec_conv1(x))\n","    x = self.dec_bn1(x)\n","    x = self.dec_upsamp2(x) \n","    x = torch.cat([x, sc_feat8], 1)\n","    x = F.relu(self.dec_conv2(x))\n","    x = self.dec_bn2(x)\n","    x = self.dec_upsamp3(x) \n","    x = torch.cat([x, sc_feat16], 1)\n","    x = F.relu(self.dec_conv3(x))\n","    x = self.dec_bn3(x)\n","    x = self.dec_upsamp4(x) \n","    x = torch.cat([x, sc_feat32], 1)\n","    x = F.relu(self.dec_conv4(x))\n","    x = self.dec_bn4(x)\n","    x = self.dec_upsamp5(x) \n","    x = F.tanh(self.dec_conv5(x))\n","    return x\n","      \n","  #define forward pass\n","  def forward(self, color, greylevel):\n","    sc_feat32, sc_feat16, sc_feat8, sc_feat4 = self.cond_encoder(greylevel)\n","    mu, logvar = self.encoder(color)\n","    \n","    stddev = torch.sqrt(torch.exp(logvar))\n","    eps = Variable(torch.randn(stddev.size()).normal_()).cuda()\n","    z = torch.add(mu, torch.mul(eps, stddev))\n","    \n","  \n","    \n","    \n","    color_out = self.decoder(z, sc_feat32, sc_feat16, sc_feat8, sc_feat4)\n","    return mu, logvar, color_out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GSqzFhZ4JYM","colab_type":"code","outputId":"0c09c9a8-4a88-4bf8-9384-6febb6296941","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import torch\n","import torchvision\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.datasets import STL10\n","import os\n","import cv2\n","import numpy as np\n","\n","if not os.path.exists('./gdrive/My Drive/Colab Notebooks/colorize'):\n","    os.mkdir('./gdrive/My Drive/Colab Notebooks/colorize')\n","\n","    \n","def rgb2gray(img):\n","  temp=img.numpy().copy()\n","  \n","  out=np.empty((batch_size,1,64,64))\n","  for i in range(batch_size):\n","    a=temp[i]\n","    c=a.transpose((1,2,0))\n","    b=cv2.cvtColor(c,cv2.COLOR_RGB2GRAY)\n","    b=np.expand_dims(b, axis=2)\n","    \n","    out[i]=b.transpose((2,0,1))\n","  return torch.from_numpy(out)\n","\n","\n","num_epochs = 100\n","batch_size = 32\n","learning_rate = 1e-4\n","\n","img_transform = transforms.Compose([transforms.Resize(64),\n","    transforms.ToTensor()\n","    \n","])\n","\n","dataset = STL10('./data',  split='unlabeled',transform=img_transform,download=True)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","\n","model = VAE()\n","\n","from collections import OrderedDict\n","\n","state_dict = torch.load('./gdrive/My Drive/Colab Notebooks/vae.pth')\n","new_state_dict = OrderedDict()\n","for k, v in state_dict.items():\n","    name = k\n","    #name = k[7:] # remove \"module.\"\n","    new_state_dict[name] = v\n","\n","model.load_state_dict(new_state_dict)\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","reconstruction_function = nn.L1Loss(size_average=False)\n","\n","\n","def loss_function(recon_x, x, mu, logvar):\n","    \"\"\"\n","    recon_x: generating images\n","    x: origin images\n","    mu: latent mean\n","    logvar: latent log variance\n","    \"\"\"\n","    BCE = reconstruction_function(recon_x, x)  # mse loss\n","    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n","    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n","    KLD = torch.sum(KLD_element).mul_(-0.5)\n","    # KL divergence\n","    return BCE + KLD\n","\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, data in enumerate(dataloader):\n","        img, _ = data\n","        gray=rgb2gray(img)\n","        gray=gray.type(torch.FloatTensor)\n","       # print(gray.size())\n","        #print(gray)\n","        \n","        img = Variable(img)\n","        gray=Variable(gray)\n","        if torch.cuda.is_available():\n","            img = img.cuda()\n","            gray=gray.cuda()\n","        optimizer.zero_grad()\n","        mu, logvar, recon_batch  = model(img,gray)\n","        loss = loss_function(recon_batch, img, mu, logvar)\n","        loss.backward()\n","        train_loss += loss.data.item()\n","        optimizer.step()\n","        if batch_idx % 500 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch,\n","                batch_idx * len(img),\n","                len(dataloader.dataset), 100. * batch_idx / len(dataloader),\n","                loss.data.item() / len(img)))\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","        epoch, train_loss / len(dataloader.dataset)))\n","    if epoch % 5 == 0:\n","        save=recon_batch.cpu().data\n","        save_image(img, './gdrive/My Drive/Colab Notebooks/colorize/GT_{}.png'.format(epoch))\n","        save_image(save, './gdrive/My Drive/Colab Notebooks/colorize/output_{}.png'.format(epoch))\n","\n","        torch.save(model.state_dict(), './gdrive/My Drive/Colab Notebooks/vae.pth')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 0 [0/100000 (0%)]\tLoss: 259.361115\n","Train Epoch: 0 [16000/100000 (16%)]\tLoss: 281.460327\n","Train Epoch: 0 [32000/100000 (32%)]\tLoss: 273.551270\n","Train Epoch: 0 [48000/100000 (48%)]\tLoss: 228.184799\n","Train Epoch: 0 [64000/100000 (64%)]\tLoss: 262.026489\n","Train Epoch: 0 [80000/100000 (80%)]\tLoss: 253.913483\n","Train Epoch: 0 [96000/100000 (96%)]\tLoss: 245.514221\n","====> Epoch: 0 Average loss: 254.5351\n","Train Epoch: 1 [0/100000 (0%)]\tLoss: 260.481506\n","Train Epoch: 1 [16000/100000 (16%)]\tLoss: 229.358734\n","Train Epoch: 1 [32000/100000 (32%)]\tLoss: 255.389893\n","Train Epoch: 1 [48000/100000 (48%)]\tLoss: 249.404480\n","Train Epoch: 1 [64000/100000 (64%)]\tLoss: 259.536041\n","Train Epoch: 1 [80000/100000 (80%)]\tLoss: 236.410980\n","Train Epoch: 1 [96000/100000 (96%)]\tLoss: 232.730011\n","====> Epoch: 1 Average loss: 253.2933\n","Train Epoch: 2 [0/100000 (0%)]\tLoss: 233.628052\n","Train Epoch: 2 [16000/100000 (16%)]\tLoss: 255.725967\n","Train Epoch: 2 [32000/100000 (32%)]\tLoss: 234.621353\n","Train Epoch: 2 [48000/100000 (48%)]\tLoss: 282.479126\n","Train Epoch: 2 [64000/100000 (64%)]\tLoss: 235.162125\n","Train Epoch: 2 [80000/100000 (80%)]\tLoss: 246.307114\n","Train Epoch: 2 [96000/100000 (96%)]\tLoss: 245.400833\n","====> Epoch: 2 Average loss: 252.2893\n","Train Epoch: 3 [0/100000 (0%)]\tLoss: 260.373352\n","Train Epoch: 3 [16000/100000 (16%)]\tLoss: 254.094955\n","Train Epoch: 3 [32000/100000 (32%)]\tLoss: 252.284180\n","Train Epoch: 3 [48000/100000 (48%)]\tLoss: 274.975067\n","Train Epoch: 3 [64000/100000 (64%)]\tLoss: 247.429733\n","Train Epoch: 3 [80000/100000 (80%)]\tLoss: 256.247803\n","Train Epoch: 3 [96000/100000 (96%)]\tLoss: 266.106018\n","====> Epoch: 3 Average loss: 250.9335\n","Train Epoch: 4 [0/100000 (0%)]\tLoss: 262.728607\n","Train Epoch: 4 [16000/100000 (16%)]\tLoss: 220.166214\n","Train Epoch: 4 [32000/100000 (32%)]\tLoss: 279.110229\n","Train Epoch: 4 [48000/100000 (48%)]\tLoss: 273.265625\n","Train Epoch: 4 [64000/100000 (64%)]\tLoss: 255.657089\n","Train Epoch: 4 [80000/100000 (80%)]\tLoss: 240.214050\n","Train Epoch: 4 [96000/100000 (96%)]\tLoss: 248.747208\n","====> Epoch: 4 Average loss: 250.6099\n","Train Epoch: 5 [0/100000 (0%)]\tLoss: 237.577682\n","Train Epoch: 5 [16000/100000 (16%)]\tLoss: 258.386536\n","Train Epoch: 5 [32000/100000 (32%)]\tLoss: 257.949799\n","Train Epoch: 5 [48000/100000 (48%)]\tLoss: 253.997467\n","Train Epoch: 5 [64000/100000 (64%)]\tLoss: 267.244568\n","Train Epoch: 5 [80000/100000 (80%)]\tLoss: 235.383713\n","Train Epoch: 5 [96000/100000 (96%)]\tLoss: 258.262329\n","====> Epoch: 5 Average loss: 251.4131\n","Train Epoch: 6 [0/100000 (0%)]\tLoss: 252.267700\n","Train Epoch: 6 [16000/100000 (16%)]\tLoss: 245.396606\n","Train Epoch: 6 [32000/100000 (32%)]\tLoss: 223.176544\n","Train Epoch: 6 [48000/100000 (48%)]\tLoss: 232.647797\n","Train Epoch: 6 [64000/100000 (64%)]\tLoss: 209.110443\n","Train Epoch: 6 [80000/100000 (80%)]\tLoss: 224.566498\n","Train Epoch: 6 [96000/100000 (96%)]\tLoss: 216.869644\n","====> Epoch: 6 Average loss: 249.9644\n","Train Epoch: 7 [0/100000 (0%)]\tLoss: 230.349777\n","Train Epoch: 7 [16000/100000 (16%)]\tLoss: 268.949219\n","Train Epoch: 7 [32000/100000 (32%)]\tLoss: 230.345413\n","Train Epoch: 7 [48000/100000 (48%)]\tLoss: 223.384399\n","Train Epoch: 7 [64000/100000 (64%)]\tLoss: 247.960571\n","Train Epoch: 7 [80000/100000 (80%)]\tLoss: 222.905121\n","Train Epoch: 7 [96000/100000 (96%)]\tLoss: 255.008194\n","====> Epoch: 7 Average loss: 247.8969\n","Train Epoch: 8 [0/100000 (0%)]\tLoss: 227.617096\n","Train Epoch: 8 [16000/100000 (16%)]\tLoss: 231.930115\n","Train Epoch: 8 [32000/100000 (32%)]\tLoss: 281.107147\n","Train Epoch: 8 [48000/100000 (48%)]\tLoss: 221.634918\n","Train Epoch: 8 [64000/100000 (64%)]\tLoss: 247.150024\n","Train Epoch: 8 [80000/100000 (80%)]\tLoss: 262.131165\n","Train Epoch: 8 [96000/100000 (96%)]\tLoss: 243.872864\n","====> Epoch: 8 Average loss: 247.3102\n","Train Epoch: 9 [0/100000 (0%)]\tLoss: 245.112167\n","Train Epoch: 9 [16000/100000 (16%)]\tLoss: 264.280151\n","Train Epoch: 9 [32000/100000 (32%)]\tLoss: 231.457413\n","Train Epoch: 9 [48000/100000 (48%)]\tLoss: 260.844452\n","Train Epoch: 9 [64000/100000 (64%)]\tLoss: 232.965286\n","Train Epoch: 9 [80000/100000 (80%)]\tLoss: 223.937943\n","Train Epoch: 9 [96000/100000 (96%)]\tLoss: 224.914215\n","====> Epoch: 9 Average loss: 246.4976\n","Train Epoch: 10 [0/100000 (0%)]\tLoss: 259.021973\n","Train Epoch: 10 [16000/100000 (16%)]\tLoss: 226.742050\n","Train Epoch: 10 [32000/100000 (32%)]\tLoss: 243.331360\n","Train Epoch: 10 [48000/100000 (48%)]\tLoss: 245.266769\n","Train Epoch: 10 [64000/100000 (64%)]\tLoss: 241.565552\n","Train Epoch: 10 [80000/100000 (80%)]\tLoss: 232.510742\n","Train Epoch: 10 [96000/100000 (96%)]\tLoss: 248.683228\n","====> Epoch: 10 Average loss: 246.5582\n","Train Epoch: 11 [0/100000 (0%)]\tLoss: 248.285583\n","Train Epoch: 11 [16000/100000 (16%)]\tLoss: 254.222015\n","Train Epoch: 11 [32000/100000 (32%)]\tLoss: 239.750397\n","Train Epoch: 11 [48000/100000 (48%)]\tLoss: 246.238571\n","Train Epoch: 11 [64000/100000 (64%)]\tLoss: 261.895996\n","Train Epoch: 11 [80000/100000 (80%)]\tLoss: 251.377365\n","Train Epoch: 11 [96000/100000 (96%)]\tLoss: 285.060608\n","====> Epoch: 11 Average loss: 257.1457\n","Train Epoch: 12 [0/100000 (0%)]\tLoss: 240.846344\n","Train Epoch: 12 [16000/100000 (16%)]\tLoss: 255.304047\n","Train Epoch: 12 [32000/100000 (32%)]\tLoss: 239.341965\n","Train Epoch: 12 [48000/100000 (48%)]\tLoss: 262.207275\n","Train Epoch: 12 [64000/100000 (64%)]\tLoss: 227.029495\n","Train Epoch: 12 [80000/100000 (80%)]\tLoss: 263.701385\n","Train Epoch: 12 [96000/100000 (96%)]\tLoss: 232.616150\n","====> Epoch: 12 Average loss: 253.8229\n","Train Epoch: 13 [0/100000 (0%)]\tLoss: 251.869080\n","Train Epoch: 13 [16000/100000 (16%)]\tLoss: 291.531586\n","Train Epoch: 13 [32000/100000 (32%)]\tLoss: 236.279968\n","Train Epoch: 13 [48000/100000 (48%)]\tLoss: 239.905365\n","Train Epoch: 13 [64000/100000 (64%)]\tLoss: 244.343445\n","Train Epoch: 13 [80000/100000 (80%)]\tLoss: 236.959747\n","Train Epoch: 13 [96000/100000 (96%)]\tLoss: 216.001251\n","====> Epoch: 13 Average loss: 244.8605\n","Train Epoch: 14 [0/100000 (0%)]\tLoss: 230.973328\n","Train Epoch: 14 [16000/100000 (16%)]\tLoss: 267.004761\n","Train Epoch: 14 [32000/100000 (32%)]\tLoss: 247.472290\n","Train Epoch: 14 [48000/100000 (48%)]\tLoss: 268.078033\n","Train Epoch: 14 [64000/100000 (64%)]\tLoss: 255.462006\n","Train Epoch: 14 [80000/100000 (80%)]\tLoss: 240.399887\n","Train Epoch: 14 [96000/100000 (96%)]\tLoss: 247.782333\n","====> Epoch: 14 Average loss: 244.0773\n","Train Epoch: 15 [0/100000 (0%)]\tLoss: 275.130920\n","Train Epoch: 15 [16000/100000 (16%)]\tLoss: 205.835358\n","Train Epoch: 15 [32000/100000 (32%)]\tLoss: 231.991959\n","Train Epoch: 15 [48000/100000 (48%)]\tLoss: 233.588989\n","Train Epoch: 15 [64000/100000 (64%)]\tLoss: 237.496460\n","Train Epoch: 15 [80000/100000 (80%)]\tLoss: 239.363617\n","Train Epoch: 15 [96000/100000 (96%)]\tLoss: 235.001160\n","====> Epoch: 15 Average loss: 243.3219\n","Train Epoch: 16 [0/100000 (0%)]\tLoss: 268.355682\n","Train Epoch: 16 [16000/100000 (16%)]\tLoss: 242.116425\n","Train Epoch: 16 [32000/100000 (32%)]\tLoss: 239.279861\n","Train Epoch: 16 [48000/100000 (48%)]\tLoss: 245.647095\n","Train Epoch: 16 [64000/100000 (64%)]\tLoss: 260.599274\n","Train Epoch: 16 [80000/100000 (80%)]\tLoss: 239.567276\n","Train Epoch: 16 [96000/100000 (96%)]\tLoss: 252.749176\n","====> Epoch: 16 Average loss: 243.3581\n","Train Epoch: 17 [0/100000 (0%)]\tLoss: 236.833710\n","Train Epoch: 17 [16000/100000 (16%)]\tLoss: 249.525665\n","Train Epoch: 17 [32000/100000 (32%)]\tLoss: 232.675690\n","Train Epoch: 17 [48000/100000 (48%)]\tLoss: 249.671783\n","Train Epoch: 17 [64000/100000 (64%)]\tLoss: 250.012161\n","Train Epoch: 17 [80000/100000 (80%)]\tLoss: 256.134460\n","Train Epoch: 17 [96000/100000 (96%)]\tLoss: 224.139954\n","====> Epoch: 17 Average loss: 241.5270\n","Train Epoch: 18 [0/100000 (0%)]\tLoss: 273.829041\n","Train Epoch: 18 [16000/100000 (16%)]\tLoss: 248.162872\n","Train Epoch: 18 [32000/100000 (32%)]\tLoss: 237.591904\n","Train Epoch: 18 [48000/100000 (48%)]\tLoss: 233.162857\n","Train Epoch: 18 [64000/100000 (64%)]\tLoss: 237.803528\n","Train Epoch: 18 [80000/100000 (80%)]\tLoss: 240.883408\n","Train Epoch: 18 [96000/100000 (96%)]\tLoss: 275.249115\n","====> Epoch: 18 Average loss: 242.1476\n","Train Epoch: 19 [0/100000 (0%)]\tLoss: 238.706772\n","Train Epoch: 19 [16000/100000 (16%)]\tLoss: 271.252502\n","Train Epoch: 19 [32000/100000 (32%)]\tLoss: 243.760956\n","Train Epoch: 19 [48000/100000 (48%)]\tLoss: 220.498596\n","Train Epoch: 19 [64000/100000 (64%)]\tLoss: 254.821213\n","Train Epoch: 19 [80000/100000 (80%)]\tLoss: 247.918091\n","Train Epoch: 19 [96000/100000 (96%)]\tLoss: 230.395477\n","====> Epoch: 19 Average loss: 241.6843\n","Train Epoch: 20 [0/100000 (0%)]\tLoss: 228.163208\n","Train Epoch: 20 [16000/100000 (16%)]\tLoss: 243.977509\n","Train Epoch: 20 [32000/100000 (32%)]\tLoss: 235.179886\n","Train Epoch: 20 [48000/100000 (48%)]\tLoss: 236.736023\n","Train Epoch: 20 [64000/100000 (64%)]\tLoss: 273.652435\n","Train Epoch: 20 [80000/100000 (80%)]\tLoss: 239.048492\n","Train Epoch: 20 [96000/100000 (96%)]\tLoss: 263.637115\n","====> Epoch: 20 Average loss: 239.4605\n","Train Epoch: 21 [0/100000 (0%)]\tLoss: 246.948090\n","Train Epoch: 21 [16000/100000 (16%)]\tLoss: 223.303894\n","Train Epoch: 21 [32000/100000 (32%)]\tLoss: 221.926193\n","Train Epoch: 21 [48000/100000 (48%)]\tLoss: 258.190887\n","Train Epoch: 21 [64000/100000 (64%)]\tLoss: 243.423279\n","Train Epoch: 21 [80000/100000 (80%)]\tLoss: 262.112122\n","Train Epoch: 21 [96000/100000 (96%)]\tLoss: 239.170288\n","====> Epoch: 21 Average loss: 238.7823\n","Train Epoch: 22 [0/100000 (0%)]\tLoss: 247.357742\n","Train Epoch: 22 [16000/100000 (16%)]\tLoss: 248.513306\n","Train Epoch: 22 [32000/100000 (32%)]\tLoss: 247.989578\n","Train Epoch: 22 [48000/100000 (48%)]\tLoss: 227.803940\n","Train Epoch: 22 [64000/100000 (64%)]\tLoss: 228.071625\n","Train Epoch: 22 [80000/100000 (80%)]\tLoss: 228.561905\n","Train Epoch: 22 [96000/100000 (96%)]\tLoss: 253.764496\n","====> Epoch: 22 Average loss: 238.7773\n","Train Epoch: 23 [0/100000 (0%)]\tLoss: 246.614136\n","Train Epoch: 23 [16000/100000 (16%)]\tLoss: 233.819397\n","Train Epoch: 23 [32000/100000 (32%)]\tLoss: 252.386292\n","Train Epoch: 23 [48000/100000 (48%)]\tLoss: 259.350922\n","Train Epoch: 23 [64000/100000 (64%)]\tLoss: 228.910019\n","Train Epoch: 23 [80000/100000 (80%)]\tLoss: 232.414536\n","Train Epoch: 23 [96000/100000 (96%)]\tLoss: 234.863464\n","====> Epoch: 23 Average loss: 237.7556\n","Train Epoch: 24 [0/100000 (0%)]\tLoss: 231.538086\n","Train Epoch: 24 [16000/100000 (16%)]\tLoss: 222.860535\n","Train Epoch: 24 [32000/100000 (32%)]\tLoss: 222.830734\n","Train Epoch: 24 [48000/100000 (48%)]\tLoss: 242.723557\n","Train Epoch: 24 [64000/100000 (64%)]\tLoss: 234.238907\n","Train Epoch: 24 [80000/100000 (80%)]\tLoss: 240.799088\n","Train Epoch: 24 [96000/100000 (96%)]\tLoss: 244.514404\n","====> Epoch: 24 Average loss: 237.4936\n","Train Epoch: 25 [0/100000 (0%)]\tLoss: 225.071930\n","Train Epoch: 25 [16000/100000 (16%)]\tLoss: 228.946091\n","Train Epoch: 25 [32000/100000 (32%)]\tLoss: 237.414444\n","Train Epoch: 25 [48000/100000 (48%)]\tLoss: 242.342972\n","Train Epoch: 25 [64000/100000 (64%)]\tLoss: 239.437195\n","Train Epoch: 25 [80000/100000 (80%)]\tLoss: 221.764297\n","Train Epoch: 25 [96000/100000 (96%)]\tLoss: 251.588318\n","====> Epoch: 25 Average loss: 237.0728\n","Train Epoch: 26 [0/100000 (0%)]\tLoss: 221.279648\n","Train Epoch: 26 [16000/100000 (16%)]\tLoss: 233.293793\n","Train Epoch: 26 [32000/100000 (32%)]\tLoss: 246.966003\n","Train Epoch: 26 [48000/100000 (48%)]\tLoss: 230.882187\n","Train Epoch: 26 [64000/100000 (64%)]\tLoss: 241.098816\n","Train Epoch: 26 [80000/100000 (80%)]\tLoss: 230.939117\n","Train Epoch: 26 [96000/100000 (96%)]\tLoss: 260.938171\n","====> Epoch: 26 Average loss: 235.8986\n","Train Epoch: 27 [0/100000 (0%)]\tLoss: 246.453278\n","Train Epoch: 27 [16000/100000 (16%)]\tLoss: 248.537674\n","Train Epoch: 27 [32000/100000 (32%)]\tLoss: 238.378372\n","Train Epoch: 27 [48000/100000 (48%)]\tLoss: 241.632889\n","Train Epoch: 27 [64000/100000 (64%)]\tLoss: 233.488998\n","Train Epoch: 27 [80000/100000 (80%)]\tLoss: 244.442871\n","Train Epoch: 27 [96000/100000 (96%)]\tLoss: 231.757858\n","====> Epoch: 27 Average loss: 236.5035\n","Train Epoch: 28 [0/100000 (0%)]\tLoss: 233.974670\n","Train Epoch: 28 [16000/100000 (16%)]\tLoss: 212.141754\n","Train Epoch: 28 [32000/100000 (32%)]\tLoss: 225.473145\n","Train Epoch: 28 [48000/100000 (48%)]\tLoss: 233.103027\n","Train Epoch: 28 [64000/100000 (64%)]\tLoss: 239.912735\n","Train Epoch: 28 [80000/100000 (80%)]\tLoss: 240.702957\n","Train Epoch: 28 [96000/100000 (96%)]\tLoss: 237.172089\n","====> Epoch: 28 Average loss: 235.0449\n","Train Epoch: 29 [0/100000 (0%)]\tLoss: 236.178131\n","Train Epoch: 29 [16000/100000 (16%)]\tLoss: 223.248230\n","Train Epoch: 29 [32000/100000 (32%)]\tLoss: 256.561371\n","Train Epoch: 29 [48000/100000 (48%)]\tLoss: 229.609085\n","Train Epoch: 29 [64000/100000 (64%)]\tLoss: 242.162933\n","Train Epoch: 29 [80000/100000 (80%)]\tLoss: 246.127670\n","Train Epoch: 29 [96000/100000 (96%)]\tLoss: 235.253754\n","====> Epoch: 29 Average loss: 234.2513\n","Train Epoch: 30 [0/100000 (0%)]\tLoss: 204.429016\n","Train Epoch: 30 [16000/100000 (16%)]\tLoss: 244.955536\n","Train Epoch: 30 [32000/100000 (32%)]\tLoss: 247.096420\n","Train Epoch: 30 [48000/100000 (48%)]\tLoss: 220.864517\n","Train Epoch: 30 [64000/100000 (64%)]\tLoss: 255.196976\n","Train Epoch: 30 [80000/100000 (80%)]\tLoss: 238.089096\n","Train Epoch: 30 [96000/100000 (96%)]\tLoss: 233.795959\n","====> Epoch: 30 Average loss: 233.8539\n","Train Epoch: 31 [0/100000 (0%)]\tLoss: 223.084488\n","Train Epoch: 31 [16000/100000 (16%)]\tLoss: 227.436844\n","Train Epoch: 31 [32000/100000 (32%)]\tLoss: 239.436508\n","Train Epoch: 31 [48000/100000 (48%)]\tLoss: 208.756195\n","Train Epoch: 31 [64000/100000 (64%)]\tLoss: 213.527695\n","Train Epoch: 31 [80000/100000 (80%)]\tLoss: 237.121918\n","Train Epoch: 31 [96000/100000 (96%)]\tLoss: 247.004776\n","====> Epoch: 31 Average loss: 233.2536\n","Train Epoch: 32 [0/100000 (0%)]\tLoss: 231.824890\n","Train Epoch: 32 [16000/100000 (16%)]\tLoss: 234.028351\n","Train Epoch: 32 [32000/100000 (32%)]\tLoss: 241.781235\n","Train Epoch: 32 [48000/100000 (48%)]\tLoss: 252.074677\n","Train Epoch: 32 [64000/100000 (64%)]\tLoss: 218.316345\n","Train Epoch: 32 [80000/100000 (80%)]\tLoss: 221.790466\n","Train Epoch: 32 [96000/100000 (96%)]\tLoss: 243.205505\n","====> Epoch: 32 Average loss: 232.7364\n","Train Epoch: 33 [0/100000 (0%)]\tLoss: 258.073425\n","Train Epoch: 33 [16000/100000 (16%)]\tLoss: 231.717712\n","Train Epoch: 33 [32000/100000 (32%)]\tLoss: 230.819443\n","Train Epoch: 33 [48000/100000 (48%)]\tLoss: 214.268723\n","Train Epoch: 33 [64000/100000 (64%)]\tLoss: 220.080246\n","Train Epoch: 33 [80000/100000 (80%)]\tLoss: 265.060272\n","Train Epoch: 33 [96000/100000 (96%)]\tLoss: 237.025558\n","====> Epoch: 33 Average loss: 232.4238\n","Train Epoch: 34 [0/100000 (0%)]\tLoss: 244.431656\n","Train Epoch: 34 [16000/100000 (16%)]\tLoss: 230.758804\n","Train Epoch: 34 [32000/100000 (32%)]\tLoss: 234.582001\n","Train Epoch: 34 [48000/100000 (48%)]\tLoss: 229.614410\n","Train Epoch: 34 [64000/100000 (64%)]\tLoss: 257.022827\n","Train Epoch: 34 [80000/100000 (80%)]\tLoss: 247.500061\n","Train Epoch: 34 [96000/100000 (96%)]\tLoss: 221.596695\n","====> Epoch: 34 Average loss: 232.4151\n","Train Epoch: 35 [0/100000 (0%)]\tLoss: 225.072830\n","Train Epoch: 35 [16000/100000 (16%)]\tLoss: 220.298157\n","Train Epoch: 35 [32000/100000 (32%)]\tLoss: 225.876160\n","Train Epoch: 35 [48000/100000 (48%)]\tLoss: 223.977417\n","Train Epoch: 35 [64000/100000 (64%)]\tLoss: 272.531097\n","Train Epoch: 35 [80000/100000 (80%)]\tLoss: 239.588043\n","Train Epoch: 35 [96000/100000 (96%)]\tLoss: 237.074310\n","====> Epoch: 35 Average loss: 231.3307\n","Train Epoch: 36 [0/100000 (0%)]\tLoss: 233.466278\n","Train Epoch: 36 [16000/100000 (16%)]\tLoss: 215.817368\n","Train Epoch: 36 [32000/100000 (32%)]\tLoss: 247.066071\n","Train Epoch: 36 [48000/100000 (48%)]\tLoss: 233.025040\n","Train Epoch: 36 [64000/100000 (64%)]\tLoss: 221.811981\n","Train Epoch: 36 [80000/100000 (80%)]\tLoss: 233.708160\n","Train Epoch: 36 [96000/100000 (96%)]\tLoss: 230.874329\n","====> Epoch: 36 Average loss: 230.6890\n","Train Epoch: 37 [0/100000 (0%)]\tLoss: 217.254974\n","Train Epoch: 37 [16000/100000 (16%)]\tLoss: 234.378113\n","Train Epoch: 37 [32000/100000 (32%)]\tLoss: 259.460815\n","Train Epoch: 37 [48000/100000 (48%)]\tLoss: 230.099640\n","Train Epoch: 37 [64000/100000 (64%)]\tLoss: 219.163177\n","Train Epoch: 37 [80000/100000 (80%)]\tLoss: 232.367340\n","Train Epoch: 37 [96000/100000 (96%)]\tLoss: 221.449677\n","====> Epoch: 37 Average loss: 230.1210\n","Train Epoch: 38 [0/100000 (0%)]\tLoss: 229.009644\n","Train Epoch: 38 [16000/100000 (16%)]\tLoss: 223.795898\n","Train Epoch: 38 [32000/100000 (32%)]\tLoss: 220.701553\n","Train Epoch: 38 [48000/100000 (48%)]\tLoss: 230.260757\n","Train Epoch: 38 [64000/100000 (64%)]\tLoss: 229.167450\n","Train Epoch: 38 [80000/100000 (80%)]\tLoss: 255.546555\n","Train Epoch: 38 [96000/100000 (96%)]\tLoss: 222.521515\n","====> Epoch: 38 Average loss: 229.9844\n","Train Epoch: 39 [0/100000 (0%)]\tLoss: 195.222351\n","Train Epoch: 39 [16000/100000 (16%)]\tLoss: 213.400528\n","Train Epoch: 39 [32000/100000 (32%)]\tLoss: 240.535980\n","Train Epoch: 39 [48000/100000 (48%)]\tLoss: 225.240265\n","Train Epoch: 39 [64000/100000 (64%)]\tLoss: 231.502792\n","Train Epoch: 39 [80000/100000 (80%)]\tLoss: 209.168427\n","Train Epoch: 39 [96000/100000 (96%)]\tLoss: 204.207825\n","====> Epoch: 39 Average loss: 230.0705\n","Train Epoch: 40 [0/100000 (0%)]\tLoss: 243.283585\n","Train Epoch: 40 [16000/100000 (16%)]\tLoss: 213.646454\n","Train Epoch: 40 [32000/100000 (32%)]\tLoss: 239.564972\n","Train Epoch: 40 [48000/100000 (48%)]\tLoss: 222.918335\n","Train Epoch: 40 [64000/100000 (64%)]\tLoss: 248.844437\n","Train Epoch: 40 [80000/100000 (80%)]\tLoss: 213.535690\n","Train Epoch: 40 [96000/100000 (96%)]\tLoss: 228.217209\n","====> Epoch: 40 Average loss: 229.0337\n","Train Epoch: 41 [0/100000 (0%)]\tLoss: 232.504868\n","Train Epoch: 41 [16000/100000 (16%)]\tLoss: 230.282822\n","Train Epoch: 41 [32000/100000 (32%)]\tLoss: 227.291183\n","Train Epoch: 41 [48000/100000 (48%)]\tLoss: 233.964310\n","Train Epoch: 41 [64000/100000 (64%)]\tLoss: 230.031494\n","Train Epoch: 41 [80000/100000 (80%)]\tLoss: 240.309250\n","Train Epoch: 41 [96000/100000 (96%)]\tLoss: 233.301559\n","====> Epoch: 41 Average loss: 227.9366\n","Train Epoch: 42 [0/100000 (0%)]\tLoss: 233.680252\n","Train Epoch: 42 [16000/100000 (16%)]\tLoss: 265.768982\n","Train Epoch: 42 [32000/100000 (32%)]\tLoss: 228.468628\n","Train Epoch: 42 [48000/100000 (48%)]\tLoss: 233.138031\n","Train Epoch: 42 [64000/100000 (64%)]\tLoss: 247.043518\n","Train Epoch: 42 [80000/100000 (80%)]\tLoss: 222.115952\n","Train Epoch: 42 [96000/100000 (96%)]\tLoss: 244.892944\n","====> Epoch: 42 Average loss: 228.3867\n","Train Epoch: 43 [0/100000 (0%)]\tLoss: 225.394867\n","Train Epoch: 43 [16000/100000 (16%)]\tLoss: 228.174622\n","Train Epoch: 43 [32000/100000 (32%)]\tLoss: 217.689056\n","Train Epoch: 43 [48000/100000 (48%)]\tLoss: 219.873581\n","Train Epoch: 43 [64000/100000 (64%)]\tLoss: 233.098022\n","Train Epoch: 43 [80000/100000 (80%)]\tLoss: 220.973434\n","Train Epoch: 43 [96000/100000 (96%)]\tLoss: 219.050995\n","====> Epoch: 43 Average loss: 227.1829\n","Train Epoch: 44 [0/100000 (0%)]\tLoss: 238.599075\n","Train Epoch: 44 [16000/100000 (16%)]\tLoss: 230.581055\n","Train Epoch: 44 [32000/100000 (32%)]\tLoss: 235.818192\n","Train Epoch: 44 [48000/100000 (48%)]\tLoss: 231.136337\n","Train Epoch: 44 [64000/100000 (64%)]\tLoss: 215.820160\n","Train Epoch: 44 [80000/100000 (80%)]\tLoss: 239.545349\n","Train Epoch: 44 [96000/100000 (96%)]\tLoss: 226.166901\n","====> Epoch: 44 Average loss: 226.9275\n","Train Epoch: 45 [0/100000 (0%)]\tLoss: 233.350525\n","Train Epoch: 45 [16000/100000 (16%)]\tLoss: 223.619568\n","Train Epoch: 45 [32000/100000 (32%)]\tLoss: 252.125366\n","Train Epoch: 45 [48000/100000 (48%)]\tLoss: 202.738510\n","Train Epoch: 45 [64000/100000 (64%)]\tLoss: 209.370590\n","Train Epoch: 45 [80000/100000 (80%)]\tLoss: 223.450272\n","Train Epoch: 45 [96000/100000 (96%)]\tLoss: 233.619232\n","====> Epoch: 45 Average loss: 226.7316\n","Train Epoch: 46 [0/100000 (0%)]\tLoss: 219.085358\n","Train Epoch: 46 [16000/100000 (16%)]\tLoss: 223.445343\n","Train Epoch: 46 [32000/100000 (32%)]\tLoss: 233.136505\n","Train Epoch: 46 [48000/100000 (48%)]\tLoss: 254.787262\n","Train Epoch: 46 [64000/100000 (64%)]\tLoss: 226.734772\n","Train Epoch: 46 [80000/100000 (80%)]\tLoss: 213.878342\n","Train Epoch: 46 [96000/100000 (96%)]\tLoss: 278.408478\n","====> Epoch: 46 Average loss: 229.1589\n","Train Epoch: 47 [0/100000 (0%)]\tLoss: 224.383224\n","Train Epoch: 47 [16000/100000 (16%)]\tLoss: 216.239075\n","Train Epoch: 47 [32000/100000 (32%)]\tLoss: 242.871429\n","Train Epoch: 47 [48000/100000 (48%)]\tLoss: 221.273254\n","Train Epoch: 47 [64000/100000 (64%)]\tLoss: 232.467941\n","Train Epoch: 47 [80000/100000 (80%)]\tLoss: 201.660675\n","Train Epoch: 47 [96000/100000 (96%)]\tLoss: 246.162979\n","====> Epoch: 47 Average loss: 228.5903\n","Train Epoch: 48 [0/100000 (0%)]\tLoss: 226.956894\n","Train Epoch: 48 [16000/100000 (16%)]\tLoss: 232.894745\n","Train Epoch: 48 [32000/100000 (32%)]\tLoss: 223.577393\n","Train Epoch: 48 [48000/100000 (48%)]\tLoss: 223.440948\n","Train Epoch: 48 [64000/100000 (64%)]\tLoss: 206.119583\n","Train Epoch: 48 [80000/100000 (80%)]\tLoss: 214.400986\n","Train Epoch: 48 [96000/100000 (96%)]\tLoss: 240.031708\n","====> Epoch: 48 Average loss: 225.2540\n","Train Epoch: 49 [0/100000 (0%)]\tLoss: 209.252853\n","Train Epoch: 49 [16000/100000 (16%)]\tLoss: 223.675079\n","Train Epoch: 49 [32000/100000 (32%)]\tLoss: 241.949097\n","Train Epoch: 49 [48000/100000 (48%)]\tLoss: 217.387024\n","Train Epoch: 49 [64000/100000 (64%)]\tLoss: 217.490967\n","Train Epoch: 49 [80000/100000 (80%)]\tLoss: 203.986862\n","Train Epoch: 49 [96000/100000 (96%)]\tLoss: 224.948547\n","====> Epoch: 49 Average loss: 225.5209\n","Train Epoch: 50 [0/100000 (0%)]\tLoss: 228.625458\n","Train Epoch: 50 [16000/100000 (16%)]\tLoss: 224.138138\n","Train Epoch: 50 [32000/100000 (32%)]\tLoss: 212.778061\n","Train Epoch: 50 [48000/100000 (48%)]\tLoss: 247.607895\n","Train Epoch: 50 [64000/100000 (64%)]\tLoss: 242.569000\n","Train Epoch: 50 [80000/100000 (80%)]\tLoss: 212.347656\n","Train Epoch: 50 [96000/100000 (96%)]\tLoss: 228.815643\n","====> Epoch: 50 Average loss: 225.6149\n","Train Epoch: 51 [0/100000 (0%)]\tLoss: 241.952774\n","Train Epoch: 51 [16000/100000 (16%)]\tLoss: 237.586899\n","Train Epoch: 51 [32000/100000 (32%)]\tLoss: 228.579727\n","Train Epoch: 51 [48000/100000 (48%)]\tLoss: 236.113708\n","Train Epoch: 51 [64000/100000 (64%)]\tLoss: 213.148956\n","Train Epoch: 51 [80000/100000 (80%)]\tLoss: 234.532166\n","Train Epoch: 51 [96000/100000 (96%)]\tLoss: 231.907791\n","====> Epoch: 51 Average loss: 224.9964\n","Train Epoch: 52 [0/100000 (0%)]\tLoss: 226.186172\n","Train Epoch: 52 [16000/100000 (16%)]\tLoss: 234.199173\n","Train Epoch: 52 [32000/100000 (32%)]\tLoss: 231.210449\n","Train Epoch: 52 [48000/100000 (48%)]\tLoss: 216.150604\n","Train Epoch: 52 [64000/100000 (64%)]\tLoss: 223.225342\n","Train Epoch: 52 [80000/100000 (80%)]\tLoss: 222.913742\n","Train Epoch: 52 [96000/100000 (96%)]\tLoss: 252.085892\n","====> Epoch: 52 Average loss: 225.0180\n","Train Epoch: 53 [0/100000 (0%)]\tLoss: 223.690796\n","Train Epoch: 53 [16000/100000 (16%)]\tLoss: 196.507584\n","Train Epoch: 53 [32000/100000 (32%)]\tLoss: 236.896072\n","Train Epoch: 53 [48000/100000 (48%)]\tLoss: 228.521851\n","Train Epoch: 53 [64000/100000 (64%)]\tLoss: 227.223083\n","Train Epoch: 53 [80000/100000 (80%)]\tLoss: 209.716858\n","Train Epoch: 53 [96000/100000 (96%)]\tLoss: 223.291412\n","====> Epoch: 53 Average loss: 224.1437\n","Train Epoch: 54 [0/100000 (0%)]\tLoss: 213.401169\n","Train Epoch: 54 [16000/100000 (16%)]\tLoss: 238.487610\n","Train Epoch: 54 [32000/100000 (32%)]\tLoss: 243.183197\n","Train Epoch: 54 [48000/100000 (48%)]\tLoss: 230.395966\n","Train Epoch: 54 [64000/100000 (64%)]\tLoss: 244.229538\n","Train Epoch: 54 [80000/100000 (80%)]\tLoss: 249.616272\n","Train Epoch: 54 [96000/100000 (96%)]\tLoss: 227.190826\n","====> Epoch: 54 Average loss: 224.7649\n","Train Epoch: 55 [0/100000 (0%)]\tLoss: 227.078339\n","Train Epoch: 55 [16000/100000 (16%)]\tLoss: 237.194504\n","Train Epoch: 55 [32000/100000 (32%)]\tLoss: 240.401917\n","Train Epoch: 55 [48000/100000 (48%)]\tLoss: 219.945465\n","Train Epoch: 55 [64000/100000 (64%)]\tLoss: 224.972778\n","Train Epoch: 55 [80000/100000 (80%)]\tLoss: 223.902588\n","Train Epoch: 55 [96000/100000 (96%)]\tLoss: 224.865555\n","====> Epoch: 55 Average loss: 225.8573\n","Train Epoch: 56 [0/100000 (0%)]\tLoss: 246.942200\n","Train Epoch: 56 [16000/100000 (16%)]\tLoss: 250.007355\n","Train Epoch: 56 [32000/100000 (32%)]\tLoss: 236.020584\n","Train Epoch: 56 [48000/100000 (48%)]\tLoss: 196.556213\n","Train Epoch: 56 [64000/100000 (64%)]\tLoss: 245.202667\n","Train Epoch: 56 [80000/100000 (80%)]\tLoss: 230.488617\n","Train Epoch: 56 [96000/100000 (96%)]\tLoss: 232.871201\n","====> Epoch: 56 Average loss: 225.1473\n","Train Epoch: 57 [0/100000 (0%)]\tLoss: 232.151932\n","Train Epoch: 57 [16000/100000 (16%)]\tLoss: 227.183472\n","Train Epoch: 57 [32000/100000 (32%)]\tLoss: 216.927917\n","Train Epoch: 57 [48000/100000 (48%)]\tLoss: 209.552994\n","Train Epoch: 57 [64000/100000 (64%)]\tLoss: 236.291214\n","Train Epoch: 57 [80000/100000 (80%)]\tLoss: 236.995316\n","Train Epoch: 57 [96000/100000 (96%)]\tLoss: 219.692307\n","====> Epoch: 57 Average loss: 223.7689\n","Train Epoch: 58 [0/100000 (0%)]\tLoss: 214.677170\n","Train Epoch: 58 [16000/100000 (16%)]\tLoss: 238.945435\n","Train Epoch: 58 [32000/100000 (32%)]\tLoss: 206.611267\n","Train Epoch: 58 [48000/100000 (48%)]\tLoss: 216.362137\n","Train Epoch: 58 [64000/100000 (64%)]\tLoss: 235.728287\n","Train Epoch: 58 [80000/100000 (80%)]\tLoss: 219.409668\n","Train Epoch: 58 [96000/100000 (96%)]\tLoss: 219.006012\n","====> Epoch: 58 Average loss: 222.9696\n","Train Epoch: 59 [0/100000 (0%)]\tLoss: 236.207092\n","Train Epoch: 59 [16000/100000 (16%)]\tLoss: 225.844925\n","Train Epoch: 59 [32000/100000 (32%)]\tLoss: 211.787613\n","Train Epoch: 59 [48000/100000 (48%)]\tLoss: 229.076538\n","Train Epoch: 59 [64000/100000 (64%)]\tLoss: 207.200165\n","Train Epoch: 59 [80000/100000 (80%)]\tLoss: 220.033844\n","Train Epoch: 59 [96000/100000 (96%)]\tLoss: 226.580185\n","====> Epoch: 59 Average loss: 221.7969\n","Train Epoch: 60 [0/100000 (0%)]\tLoss: 207.565857\n","Train Epoch: 60 [16000/100000 (16%)]\tLoss: 203.174103\n","Train Epoch: 60 [32000/100000 (32%)]\tLoss: 219.859085\n","Train Epoch: 60 [48000/100000 (48%)]\tLoss: 225.053680\n","Train Epoch: 60 [64000/100000 (64%)]\tLoss: 213.079224\n","Train Epoch: 60 [80000/100000 (80%)]\tLoss: 225.137009\n","Train Epoch: 60 [96000/100000 (96%)]\tLoss: 225.542328\n","====> Epoch: 60 Average loss: 221.7482\n","Train Epoch: 61 [0/100000 (0%)]\tLoss: 215.045959\n","Train Epoch: 61 [16000/100000 (16%)]\tLoss: 229.643677\n","Train Epoch: 61 [32000/100000 (32%)]\tLoss: 210.542160\n","Train Epoch: 61 [48000/100000 (48%)]\tLoss: 229.553894\n","Train Epoch: 61 [64000/100000 (64%)]\tLoss: 217.246445\n","Train Epoch: 61 [80000/100000 (80%)]\tLoss: 225.603821\n","Train Epoch: 61 [96000/100000 (96%)]\tLoss: 234.472748\n","====> Epoch: 61 Average loss: 221.9055\n","Train Epoch: 62 [0/100000 (0%)]\tLoss: 244.156570\n","Train Epoch: 62 [16000/100000 (16%)]\tLoss: 220.873398\n","Train Epoch: 62 [32000/100000 (32%)]\tLoss: 224.353577\n","Train Epoch: 62 [48000/100000 (48%)]\tLoss: 226.037552\n","Train Epoch: 62 [64000/100000 (64%)]\tLoss: 229.254211\n","Train Epoch: 62 [80000/100000 (80%)]\tLoss: 264.209015\n","Train Epoch: 62 [96000/100000 (96%)]\tLoss: 212.746552\n","====> Epoch: 62 Average loss: 220.6339\n","Train Epoch: 63 [0/100000 (0%)]\tLoss: 226.054352\n","Train Epoch: 63 [16000/100000 (16%)]\tLoss: 238.651810\n","Train Epoch: 63 [32000/100000 (32%)]\tLoss: 216.321457\n","Train Epoch: 63 [48000/100000 (48%)]\tLoss: 240.862518\n","Train Epoch: 63 [64000/100000 (64%)]\tLoss: 242.865814\n","Train Epoch: 63 [80000/100000 (80%)]\tLoss: 220.738571\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4XQ8GeJU-T-f","colab_type":"code","outputId":"d3b3f43a-cb90-4970-ae67-cc00143f5bea","executionInfo":{"status":"ok","timestamp":1565613164954,"user_tz":-540,"elapsed":15149,"user":{"displayName":"seunghyung lee","photoUrl":"","userId":"13129293040216281247"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["import torch #==inference code==================================================\n","import torchvision\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.datasets import STL10\n","import os\n","import cv2\n","import numpy as np\n","\n","if not os.path.exists('./gdrive/My Drive/Colab Notebooks/colorize/test'):\n","    os.mkdir('./gdrive/My Drive/Colab Notebooks/colorize/test')\n","\n","    \n","def rgb2gray(img):\n","  temp=img.numpy().copy()\n","  \n","  out=np.empty((batch_size,1,64,64))\n","  for i in range(batch_size):\n","    a=temp[i]\n","    c=a.transpose((1,2,0))\n","    b=cv2.cvtColor(c,cv2.COLOR_RGB2GRAY)\n","    b=np.expand_dims(b, axis=2)\n","    \n","    out[i]=b.transpose((2,0,1))\n","  return torch.from_numpy(out)\n","\n","\n","num_epochs = 5\n","batch_size = 32\n","learning_rate = 1e-4\n","\n","img_transform = transforms.Compose([transforms.Resize(64),\n","    transforms.ToTensor()\n","    \n","])\n","\n","dataset = STL10('./data',  split='train',transform=img_transform,download=True)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","\n","model = VAE()\n","\n","from collections import OrderedDict\n","\n","state_dict = torch.load('./gdrive/My Drive/Colab Notebooks/vae.pth')\n","new_state_dict = OrderedDict()\n","for k, v in state_dict.items():\n","    name = k\n","    #name = k[7:] # remove \"module.\"\n","    new_state_dict[name] = v\n","\n","model.load_state_dict(new_state_dict)\n","\n","if torch.cuda.is_available():\n","    model.cuda()\n","\n","\n","\n","\n","for epoch in range(num_epochs):\n","    model.eval()\n","    z=torch.randn(batch_size,256)\n","    rand=DataLoader(z, batch_size=batch_size, shuffle=True)\n","    for data,z in zip(dataloader,rand):\n","        img, _ = data\n","        gray=rgb2gray(img)\n","        gray=gray.type(torch.FloatTensor)\n","        \n","        img = Variable(img)\n","        gray=Variable(gray)\n","        z=Variable(z)\n","      \n","        if torch.cuda.is_available():\n","            img = img.cuda()\n","            gray=gray.cuda()\n","            z=z.cuda()\n","        \n","        sc_feat32, sc_feat16, sc_feat8, sc_feat4  = model.cond_encoder(gray)\n","        recon_batch=model.decoder(z, sc_feat32, sc_feat16, sc_feat8, sc_feat4)\n","        \n","    if epoch % 1 == 0:\n","        save=recon_batch.cpu().data\n","        if epoch==0:\n","          save_image(img, './gdrive/My Drive/Colab Notebooks/colorize/test/test_GT.png')\n","          save_image(gray, './gdrive/My Drive/Colab Notebooks/colorize/test/test_input.png')\n","        save_image(save, './gdrive/My Drive/Colab Notebooks/colorize/test/test_output_{}.png'.format(epoch))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode))\n","/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]}]}